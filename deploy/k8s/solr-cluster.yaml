---
# ZooKeeper Headless Service for cluster communication
apiVersion: v1
kind: Service
metadata:
  name: zookeeper-headless
  namespace: digest
  labels:
    app: zookeeper
spec:
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
  - name: client
    port: 2181
  - name: peer
    port: 2888
  - name: leader-election
    port: 3888
  selector:
    app: zookeeper
---
# ZooKeeper Client Service
apiVersion: v1
kind: Service
metadata:
  name: zookeeper
  namespace: digest
  labels:
    app: zookeeper
spec:
  ports:
  - name: client
    port: 2181
  selector:
    app: zookeeper
---
# ZooKeeper StatefulSet
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: zookeeper
  namespace: digest
spec:
  serviceName: zookeeper-headless
  replicas: 3
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app: zookeeper
  template:
    metadata:
      labels:
        app: zookeeper
    spec:
      containers:
      - name: zookeeper
        image: zookeeper:3.9
        ports:
        - containerPort: 2181
          name: client
        - containerPort: 2888
          name: peer
        - containerPort: 3888
          name: leader-election
        env:
        - name: ZOO_MY_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: ZOO_SERVERS
          value: "server.1=zookeeper-0.zookeeper-headless.digest.svc.cluster.local:2888:3888;2181 server.2=zookeeper-1.zookeeper-headless.digest.svc.cluster.local:2888:3888;2181 server.3=zookeeper-2.zookeeper-headless.digest.svc.cluster.local:2888:3888;2181"
        - name: ZOO_4LW_COMMANDS_WHITELIST
          value: "stat,ruok,conf,srvr"
        command:
        - sh
        - -c
        - |
          # Extract pod ordinal from hostname
          HOST=$(hostname -s)
          ORDINAL=${HOST##*-}
          MY_ID=$((ORDINAL + 1))
          echo $MY_ID > /data/myid
          exec /docker-entrypoint.sh zkServer.sh start-foreground
        volumeMounts:
        - name: data
          mountPath: /data
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          exec:
            command:
            - sh
            - -c
            - 'echo ruok | nc localhost 2181 | grep imok'
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          exec:
            command:
            - sh
            - -c
            - 'echo stat | nc localhost 2181 | grep Mode'
          initialDelaySeconds: 10
          periodSeconds: 10
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 5Gi
---
# Solr ConfigMap for schema and config
apiVersion: v1
kind: ConfigMap
metadata:
  name: solr-config
  namespace: digest
data:
  schema.xml: |
    <?xml version="1.0" encoding="UTF-8" ?>
    <schema name="news" version="1.6">
      <!-- Document ID: SHA256 hash of canonical URL -->
      <uniqueKey>id</uniqueKey>

      <field name="id" type="string" indexed="true" stored="true" required="true"/>
      <field name="_version_" type="plong" indexed="false" stored="false"/>
      <field name="source" type="string" indexed="true" stored="true" default="web"/>
      <field name="url" type="string" indexed="true" stored="true"/>
      <field name="url_canonical" type="string" indexed="true" stored="true"/>
      <field name="domain" type="string" indexed="true" stored="true"/>
      <field name="language" type="string" indexed="true" stored="true" default="unknown"/>
      <field name="published_at" type="pdate" indexed="true" stored="true"/>
      <field name="crawled_at" type="pdate" indexed="true" stored="true"/>

      <!-- Crawl queue fields (unified: URL queue + indexed content) -->
      <field name="crawl_status" type="string" indexed="true" stored="true" default="pending"/>
      <field name="crawl_claimed_at" type="pdate" indexed="true" stored="true"/>
      <field name="crawl_claimed_by" type="string" indexed="true" stored="true"/>
      <field name="crawl_depth" type="pint" indexed="true" stored="true" default="0"/>
      <field name="crawl_error" type="string" indexed="true" stored="true"/>

      <!-- Text fields with general analyzer (stored for display) -->
      <field name="title" type="text_general" indexed="true" stored="true"/>
      <field name="description" type="text_general" indexed="true" stored="true"/>
      <field name="content" type="text_general" indexed="true" stored="true"/>

      <!-- Combined search field -->
      <field name="text" type="text_general" indexed="true" stored="false" multiValued="true"/>
      <copyField source="title" dest="text"/>
      <copyField source="content" dest="text"/>
      <copyField source="description" dest="text"/>

      <!-- Telegram-specific fields -->
      <field name="tg_peer_id" type="plong" indexed="true" stored="true"/>
      <field name="tg_channel_username" type="string" indexed="true" stored="true"/>
      <field name="tg_message_id" type="pint" indexed="true" stored="true"/>
      <field name="tg_views" type="pint" indexed="true" stored="true"/>
      <field name="tg_forwards" type="pint" indexed="true" stored="true"/>

      <!-- Dynamic fields for language-specific indexing (populated by crawler) -->
      <dynamicField name="*_en" type="text_en" indexed="true" stored="false"/>
      <dynamicField name="*_ru" type="text_ru" indexed="true" stored="false"/>
      <dynamicField name="*_el" type="text_el" indexed="true" stored="false"/>
      <dynamicField name="*_de" type="text_de" indexed="true" stored="false"/>
      <dynamicField name="*_fr" type="text_fr" indexed="true" stored="false"/>

      <!-- Field types -->
      <fieldType name="string" class="solr.StrField" sortMissingLast="true"/>
      <fieldType name="pint" class="solr.IntPointField" docValues="true"/>
      <fieldType name="plong" class="solr.LongPointField" docValues="true"/>
      <fieldType name="pdate" class="solr.DatePointField" docValues="true"/>

      <!-- General text: simple analysis without language-specific processing -->
      <fieldType name="text_general" class="solr.TextField" positionIncrementGap="100">
        <analyzer>
          <tokenizer class="solr.StandardTokenizerFactory"/>
          <filter class="solr.LowerCaseFilterFactory"/>
        </analyzer>
      </fieldType>

      <!-- English text with stopwords and stemming -->
      <fieldType name="text_en" class="solr.TextField" positionIncrementGap="100">
        <analyzer>
          <tokenizer class="solr.StandardTokenizerFactory"/>
          <filter class="solr.LowerCaseFilterFactory"/>
          <filter class="solr.StopFilterFactory" words="lang/stopwords_en.txt" ignoreCase="true"/>
          <filter class="solr.EnglishPossessiveFilterFactory"/>
          <filter class="solr.PorterStemFilterFactory"/>
        </analyzer>
      </fieldType>

      <!-- Russian text with stopwords and stemming -->
      <fieldType name="text_ru" class="solr.TextField" positionIncrementGap="100">
        <analyzer>
          <tokenizer class="solr.StandardTokenizerFactory"/>
          <filter class="solr.LowerCaseFilterFactory"/>
          <filter class="solr.StopFilterFactory" words="lang/stopwords_ru.txt" ignoreCase="true"/>
          <filter class="solr.SnowballPorterFilterFactory" language="Russian"/>
        </analyzer>
      </fieldType>

      <!-- Greek text with stopwords and stemming -->
      <fieldType name="text_el" class="solr.TextField" positionIncrementGap="100">
        <analyzer>
          <tokenizer class="solr.StandardTokenizerFactory"/>
          <filter class="solr.LowerCaseFilterFactory"/>
          <filter class="solr.StopFilterFactory" words="lang/stopwords_el.txt" ignoreCase="true"/>
          <filter class="solr.GreekStemFilterFactory"/>
        </analyzer>
      </fieldType>

      <!-- German text with stopwords and stemming -->
      <fieldType name="text_de" class="solr.TextField" positionIncrementGap="100">
        <analyzer>
          <tokenizer class="solr.StandardTokenizerFactory"/>
          <filter class="solr.LowerCaseFilterFactory"/>
          <filter class="solr.StopFilterFactory" words="lang/stopwords_de.txt" ignoreCase="true"/>
          <filter class="solr.GermanNormalizationFilterFactory"/>
          <filter class="solr.GermanLightStemFilterFactory"/>
        </analyzer>
      </fieldType>

      <!-- French text with stopwords and stemming -->
      <fieldType name="text_fr" class="solr.TextField" positionIncrementGap="100">
        <analyzer>
          <tokenizer class="solr.StandardTokenizerFactory"/>
          <filter class="solr.ElisionFilterFactory" articles="lang/contractions_fr.txt"/>
          <filter class="solr.LowerCaseFilterFactory"/>
          <filter class="solr.StopFilterFactory" words="lang/stopwords_fr.txt" ignoreCase="true"/>
          <filter class="solr.FrenchLightStemFilterFactory"/>
        </analyzer>
      </fieldType>
    </schema>
  solrconfig.xml: |
    <?xml version="1.0" encoding="UTF-8" ?>
    <config>
      <luceneMatchVersion>9.4</luceneMatchVersion>
      <dataDir>${solr.data.dir:}</dataDir>
      <directoryFactory name="DirectoryFactory" class="${solr.directoryFactory:solr.NRTCachingDirectoryFactory}"/>
      <schemaFactory class="ClassicIndexSchemaFactory"/>
      <codecFactory class="solr.SchemaCodecFactory"/>

      <!-- Required for SolrCloud replication -->
      <updateHandler class="solr.DirectUpdateHandler2">
        <updateLog>
          <str name="dir">${solr.ulog.dir:}</str>
        </updateLog>
        <autoCommit>
          <maxTime>15000</maxTime>
          <openSearcher>false</openSearcher>
        </autoCommit>
        <autoSoftCommit>
          <maxTime>1000</maxTime>
        </autoSoftCommit>
      </updateHandler>

      <requestHandler name="/select" class="solr.SearchHandler">
        <lst name="defaults">
          <str name="echoParams">explicit</str>
          <str name="wt">json</str>
          <str name="indent">true</str>
          <str name="df">text</str>
          <str name="defType">edismax</str>
          <str name="qf">title^2.0 description^1.5 content^1.0</str>
        </lst>
      </requestHandler>

      <requestHandler name="/update" class="solr.UpdateRequestHandler"/>

      <requestHandler name="/get" class="solr.RealTimeGetHandler">
        <lst name="defaults">
          <str name="omitHeader">true</str>
        </lst>
      </requestHandler>

      <requestHandler name="/admin/ping" class="solr.PingRequestHandler">
        <lst name="invariants">
          <str name="q">*:*</str>
        </lst>
      </requestHandler>
    </config>
---
# Solr Headless Service
apiVersion: v1
kind: Service
metadata:
  name: solr-headless
  namespace: digest
  labels:
    app: solr
spec:
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
  - name: solr
    port: 8983
  selector:
    app: solr
---
# Solr Service
apiVersion: v1
kind: Service
metadata:
  name: solr
  namespace: digest
  labels:
    app: solr
spec:
  ports:
  - name: solr
    port: 8983
  selector:
    app: solr
---
# Solr StatefulSet
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: solr
  namespace: digest
spec:
  serviceName: solr-headless
  replicas: 3
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app: solr
  template:
    metadata:
      labels:
        app: solr
    spec:
      enableServiceLinks: false
      securityContext:
        fsGroup: 8983
        runAsUser: 8983
        runAsGroup: 8983
      initContainers:
      - name: init-zk
        image: busybox:1.36
        command:
        - sh
        - -c
        - |
          until nc -z zookeeper 2181; do
            echo "Waiting for ZooKeeper..."
            sleep 5
          done
          echo "ZooKeeper is ready"
      - name: upload-config
        image: solr:9.4
        command:
        - bash
        - -c
        - |
          set -e
          HOST=$(hostname -s)
          # Only upload from solr-0 to avoid conflicts
          if [ "$HOST" != "solr-0" ]; then
            echo "Skipping config upload on $HOST"
            exit 0
          fi

          echo "Uploading configset from solr-0..."

          # Create temp configset directory with lang/ subdirectory
          mkdir -p /tmp/newsconf/lang
          cp /config/schema.xml /tmp/newsconf/
          cp /config/solrconfig.xml /tmp/newsconf/

          # Copy stopword files from Solr's built-in language resources
          # These are referenced in schema.xml text_en/text_ru/text_el field types
          cp /opt/solr/server/solr/configsets/_default/conf/lang/stopwords_en.txt /tmp/newsconf/lang/

          # Russian stopwords (may not be in default Solr, fetch from Lucene if needed)
          if [ -f /opt/solr/server/solr/configsets/_default/conf/lang/stopwords_ru.txt ]; then
            cp /opt/solr/server/solr/configsets/_default/conf/lang/stopwords_ru.txt /tmp/newsconf/lang/
          else
            curl -sL "https://raw.githubusercontent.com/apache/lucene/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/ru/stopwords.txt" > /tmp/newsconf/lang/stopwords_ru.txt
          fi

          # Greek stopwords (not in default Solr, fetch from Lucene)
          curl -sL "https://raw.githubusercontent.com/apache/lucene/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/el/stopwords.txt" > /tmp/newsconf/lang/stopwords_el.txt

          # German stopwords
          if [ -f /opt/solr/server/solr/configsets/_default/conf/lang/stopwords_de.txt ]; then
            cp /opt/solr/server/solr/configsets/_default/conf/lang/stopwords_de.txt /tmp/newsconf/lang/
          else
            curl -sL "https://raw.githubusercontent.com/apache/lucene/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/de/stopwords.txt" > /tmp/newsconf/lang/stopwords_de.txt
          fi

          # French stopwords
          if [ -f /opt/solr/server/solr/configsets/_default/conf/lang/stopwords_fr.txt ]; then
            cp /opt/solr/server/solr/configsets/_default/conf/lang/stopwords_fr.txt /tmp/newsconf/lang/
          else
            curl -sL "https://raw.githubusercontent.com/apache/lucene/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/fr/stopwords.txt" > /tmp/newsconf/lang/stopwords_fr.txt
          fi

          # French contractions for ElisionFilter (l', d', etc.)
          cat > /tmp/newsconf/lang/contractions_fr.txt << 'CONTRACTIONS'
# French contractions for ElisionFilter
l
m
t
qu
n
s
j
d
c
jusqu
quoiqu
lorsqu
puisqu
CONTRACTIONS

          # Wait a bit for ZK to be fully ready
          sleep 5

          # Upload configset
          /opt/solr/bin/solr zk upconfig -z zookeeper:2181 -n news -d /tmp/newsconf

          echo "Configset uploaded successfully"
        volumeMounts:
        - name: config
          mountPath: /config
      containers:
      - name: solr
        image: solr:9.4
        ports:
        - containerPort: 8983
          name: solr
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: SOLR_JAVA_MEM
          value: "-Xms1g -Xmx2g"
        - name: ZK_HOST
          value: "zookeeper:2181"
        command:
        - bash
        - -c
        - |
          # Set SOLR_HOST to full DNS name for proper cluster communication
          export SOLR_HOST="${POD_NAME}.solr-headless.digest.svc.cluster.local"
          exec solr-foreground -c -z ${ZK_HOST}
        volumeMounts:
        - name: data
          mountPath: /var/solr/data
        resources:
          requests:
            memory: "2Gi"
            cpu: "250m"
          limits:
            memory: "4Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /solr/admin/info/system
            port: 8983
          initialDelaySeconds: 60
          periodSeconds: 20
        readinessProbe:
          httpGet:
            path: /solr/admin/info/system
            port: 8983
          initialDelaySeconds: 30
          periodSeconds: 10
      volumes:
      - name: config
        configMap:
          name: solr-config
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 20Gi
---
# Job to create the news collection
apiVersion: batch/v1
kind: Job
metadata:
  name: solr-create-collection
  namespace: digest
spec:
  backoffLimit: 10
  template:
    spec:
      restartPolicy: OnFailure
      initContainers:
      - name: wait-for-solr
        image: busybox:1.36
        command:
        - sh
        - -c
        - |
          echo "Waiting for Solr cluster to be ready..."
          until wget -qO- "http://solr:8983/solr/admin/collections?action=CLUSTERSTATUS" | grep -q '"live_nodes"'; do
            echo "Waiting for Solr..."
            sleep 10
          done
          # Wait for at least 2 nodes (minimum for replication)
          for i in 1 2 3 4 5 6 7 8 9 10; do
            LIVE=$(wget -qO- "http://solr:8983/solr/admin/collections?action=CLUSTERSTATUS" | grep -o '"live_nodes":\[[^]]*\]' | grep -o 'solr-' | wc -l)
            if [ "$LIVE" -ge 2 ]; then
              echo "Sufficient Solr nodes are live ($LIVE)"
              break
            fi
            echo "Waiting for Solr nodes ($LIVE/2)..."
            sleep 10
          done
      containers:
      - name: create-collection
        image: curlimages/curl:8.6.0
        command:
        - sh
        - -c
        - |
          echo "Checking if collection exists..."
          STATUS=$(curl -s "http://solr:8983/solr/admin/collections?action=LIST")
          if echo "$STATUS" | grep -q '"news"'; then
            echo "Collection 'news' already exists"
            exit 0
          fi

          echo "Creating collection 'news'..."
          curl -s "http://solr:8983/solr/admin/collections?action=CREATE&name=news&numShards=1&replicationFactor=2&collection.configName=news"

          echo "Verifying collection..."
          sleep 5
          curl -s "http://solr:8983/solr/admin/collections?action=CLUSTERSTATUS" | grep -q '"news"' && echo "Collection created successfully" || exit 1
---
# CronJob for cleanup
apiVersion: batch/v1
kind: CronJob
metadata:
  name: solr-cleanup
  namespace: digest
spec:
  schedule: "0 3 * * *"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
          - name: cleanup
            image: curlimages/curl:8.6.0
            command:
            - sh
            - -c
            - |
              # Delete pending URLs older than 7 days
              echo "Deleting stale pending documents..."
              curl -s "http://solr:8983/solr/news/update?commit=true" \
                -H "Content-Type: application/json" \
                -d '{"delete":{"query":"crawl_status:pending AND crawled_at:[* TO NOW-7DAYS]"}}'

              # Delete error URLs older than 30 days
              echo "Deleting old error documents..."
              curl -s "http://solr:8983/solr/news/update?commit=true" \
                -H "Content-Type: application/json" \
                -d '{"delete":{"query":"crawl_status:error AND crawled_at:[* TO NOW-30DAYS]"}}'

              # Delete done URLs older than 90 days
              echo "Deleting old done documents..."
              curl -s "http://solr:8983/solr/news/update?commit=true" \
                -H "Content-Type: application/json" \
                -d '{"delete":{"query":"crawl_status:done AND source:web AND crawled_at:[* TO NOW-90DAYS]"}}'

              echo "Cleanup complete"
